{
    "page_1": {
        "paragraph_1": [
            "Arabian Journal for Science and Engineering",
            "https://doi.org/10.1007/s13369-017-3018-9"
        ],
        "paragraph_2": [
            "R E S E A R C H A R T I C L E - S Y S T E M S E N G I N E E R I N G"
        ],
        "paragraph_3": [
            "Fuzzy Q-Learning-Based Multi-agent System for Intelligent Trafﬁc",
            "Control by a Game Theory Approach"
        ],
        "paragraph_4": [
            "Abolghasem Daeichian1 · Amir Haghani2"
        ],
        "paragraph_5": [
            "Received: 11 December 2015 / Accepted: 3 December 2017",
            "© King Fahd University of Petroleum & Minerals 2017"
        ],
        "paragraph_6": [
            "Abstract",
            "This paper introduces a multi-agent approach to adjust trafﬁc lights based on trafﬁc situation in order to reduce average delay",
            "time. In the trafﬁc model, lights of each intersection are controlled by an autonomous agent. Since decision of each agent",
            "affects neighbor agents, this approach creates a classical non-stationary environment. Thus, each agent not only needs to",
            "learn from the past experience but also has to consider decision of neighbors to overcome dynamic changes of the trafﬁc",
            "network. Fuzzy Q-learning and Game theory are employed to make policy based on previous experiences and decision of",
            "neighbor agents. Simulation results illustrate the advantage of the proposed method over ﬁxed time, fuzzy, Q-learning and",
            "fuzzy Q-learning control methods."
        ],
        "paragraph_7": [
            "Keywords Trafﬁc control · Multi-agent system · Game theory · Fuzzy Q-learning"
        ],
        "paragraph_8": [
            "1 Introduction"
        ],
        "paragraph_9": [
            "Urbanization, increasing number of vehicles, and lack of",
            "transport infrastructures have increased travel time, fuel con-",
            "sumption, and air pollution. Therefore, urban life equals with",
            "waste of time, less clean air, and acoustic pollution. Con-",
            "ventional ﬁxed trafﬁc management systems are not able to",
            "ﬁght complexity and dynamic of large trafﬁc networks. While",
            "artiﬁcial intelligence (AI) are greatly employed to develop",
            "intelligent trafﬁc systems (ITS) [6,7,19,24], multi-agent sys-",
            "tem is an approach to model ITS [25,30]. This framework",
            "consists of a population of intelligent and autonomous agents",
            "work together in an environment [27]. Trafﬁc lights [20],",
            "vehicles [3], and pedestrians [29] are considered as agents",
            "in modeling of urban trafﬁc networks. Each agent needs",
            "to learn from the past experiences which is a key point to",
            "approximate a better decision-making policy. Multi-agent",
            "model-based [32] as well as model-free [12] reinforcement",
            "learning (RL) techniques are widely used in researches on",
            "ITS [6,23]."
        ],
        "paragraph_10": [
            "B Abolghasem Daeichian"
        ],
        "paragraph_11": [
            "a-daeichian@araku.ac.ir; a.daeichian@gmail.com"
        ],
        "paragraph_12": [
            "1 Department of Electrical Engineering, Faculty of"
        ],
        "paragraph_13": [
            "Engineering, Arak University, Arak 38156-8-8349, Iran"
        ],
        "paragraph_14": [
            "2 Department of Electrical Engineering, Payam Institute of"
        ],
        "paragraph_15": [
            "Higher Education, Golpayegan, Isfahan, Iran"
        ],
        "paragraph_16": [
            "In a multitude of researches, any agent only considers its",
            "own trafﬁc state in order to determine the control policy.",
            "For example, single intersection with two phases is investi-",
            "gated in [2]. Length of vehicles queue waiting on the light",
            "is considered as state which can be measured by the agent. It",
            "decides on extend green time or change it to the next phase so",
            "that the number of vehicles waiting on the light is minimized.",
            "The results show superiority of Q-learning agent over uni-",
            "form trafﬁc ﬂows and constant-ratio trafﬁc ﬂows. In [32],",
            "trafﬁc lights are considered as agents which communicate",
            "with vehicles. The vehicles estimate their mean waiting time",
            "and transmit this time to trafﬁc light where a popular RL",
            "algorithm, namely Q-learning, is used to provide a control",
            "for trafﬁc signal scheduling. Results of this study show 22%",
            "reduction in waiting time compared to constant time lights.",
            "Multi-objective reinforcement learning is utilized to control",
            "several trafﬁc lights in [17]. Optimization goals include num-",
            "ber of stops of a vehicle, mean stopping time, and length of",
            "vehicles’ queue on the next intersection. Its results indicate",
            "that multi-RL can effectively prevent the queue spillovers",
            "under congested condition to avoid large-scale trafﬁc jams.",
            "Bull et al. [10] used learner classiﬁers to control light traf-",
            "ﬁc including 4 intersections. In this research, trafﬁc lights",
            "include two phases at each intersection, where one phase is",
            "for moving north–south and one is for east-west. Controller",
            "at each intersection obtains optimum phase time through",
            "extracting if-then rules. Its results show that performance of"
        ],
        "paragraph_17": [
            "123"
        ]
    },
    "page_2": {
        "paragraph_1": [
            "the trafﬁc light using learner classiﬁer system has improved",
            "signiﬁcantly compared to constant time trafﬁc light. In [28],",
            "the learning purpose is modeled in such a way that states",
            "indications are based on the summation of the cars wait-",
            "ing times. Obviously, the more cars information is received,",
            "the model will be more complicated and state space will be",
            "larger. This issue is one of the signiﬁcant problems of large",
            "networks. Adaptive control, which is introduced in [23],",
            "uses the approximate of a function as mapping of states to",
            "scheduling. Fuzzy inference engine is exploited to decrease",
            "systematic faults of Q-algorithm in [22]. The results demon-",
            "strate that not only learning in fuzzy framework is done faster",
            "than Q-learning but also delay in intersections is decreased",
            "considerably. A multi-agent fuzzy approach is proposed in",
            "[18], where Q-learning updates the set of rule base in fussy",
            "inference engine. In [13], a new method which has the capa-",
            "bility to estimate an incomplete model of environment is",
            "described for a given non-static environment. This method",
            "is applied in a network composed of 9 intersections. The",
            "reported results show that this method has better performance",
            "than the model-free methods and model-based methods, but",
            "could not be generalized and used in larger networks."
        ],
        "paragraph_2": [
            "In other researches, agents consider other agents in",
            "determination of their own control policy. For instance, coor-",
            "dination among agents is desired in [21] where the agents not",
            "only consider number of waiting vehicles on its own inter-",
            "section, but also they consider number of vehicles which",
            "have stopped in adjacent intersections. The RL is applied",
            "on 5 intersections within three different scenario. The over-",
            "all results show improvement in delay time. In [32], RL is",
            "used to control the trafﬁc in a grid where a type of cooper-",
            "ative learning simultaneously controls the trafﬁc signals and",
            "determines the optimal routes. One of the main drawbacks",
            "of this method is the high costs of communication and infor-",
            "mation exchange, speciﬁcally when intersections of network",
            "are increased. Cooperative RL tries to extract the knowledge",
            "from neighbor agents in a scheduling learning [26]. This",
            "method is implemented in an area of Dublin including 64",
            "intersections."
        ],
        "paragraph_3": [
            "This paper introduces a hybrid fuzzy Q-learning and Game",
            "theory method for control of trafﬁc lights in multi-agent",
            "framework. It exploits the beneﬁts of fuzziﬁcation as well as",
            "interaction with other agents. The trafﬁc network is modeled",
            "by considering an autonomous agent controls in which each",
            "intersection decides on duration of green phase. The number",
            "of vehicles in different inputs of the intersection are mea-",
            "sured by the corresponding agent. Any agent interacts with",
            "neighbor agents by getting a reward from each decision. This",
            "paper proposes that each agent fuzzify the inputs and utilizes",
            "in a fuzzy inference system for fuzzy estimation of trafﬁc",
            "model states. The agent uses a Q-learning approach modi-",
            "ﬁed by Game theory to learn from the past experiences and",
            "consider the interaction with neighbor agents. The agent gets"
        ],
        "paragraph_4": [
            "123"
        ],
        "paragraph_5": [
            "Arabian Journal for Science and Engineering"
        ],
        "paragraph_6": [
            "a reward proportional to its own trafﬁc state and a reward from",
            "each decision from neighbor agents to update its Q-learning",
            "algorithm. The neighbor reward and its weighting in Q-value",
            "update is proposed to be fuzzy in the proposed method. The",
            "proposed method is applied on a ﬁve-intersection trafﬁc net-",
            "work. The simulation results indicate that proposed method",
            "outperforms the ﬁxed time, fuzzy, Q-learning and fuzzy Q-",
            "learning control methods in the sense of average delay time.",
            "This paper is unfolds as follows. After this introduction,",
            "Q-learning and its fuzzy version are described in the next",
            "section. Section 3 is devoted to application of Game theory",
            "in ITS. Sections 4 and 5 are about problem statement and",
            "proposed solution, respectively. Simulation results are given",
            "in Sect. 6. Finally, the paper is concluded in Sect. 7."
        ],
        "paragraph_7": [
            "2 Q-Learning and Fuzzy Q-Learning"
        ],
        "paragraph_8": [
            "The objective of agents which act in dynamic environments",
            "is making optimum decisions. If the agents are not aware of",
            "rewards corresponding to various actions, selecting a proper",
            "action would be challenging. To achieve this goal, learning",
            "adjusts agents’ action selection based on collected data. Each",
            "agent tries to optimize its actions with dynamic environment",
            "via trial and error in reinforcement learning (RL). The RL is",
            "actually how different situations are mapped upon actions to",
            "receive the best results or the highest reward. In many cases,",
            "actions inﬂuence the reward of next steps as well as affect the",
            "reward of its corresponding step. There are model-based [32]",
            "as well as model-free [12] RL techniques. In model-free RL,",
            "the agent does not need explicit modeling of the environ-",
            "ment because its actions could be directly selected based on",
            "rewards. Q-learning is a model-independent approach where",
            "the agent does not access to transfer model [1,31]. Suppose",
            "that the agent is in a state s, performs an action a, from",
            "which it gets the rewards r from the environment and the",
            "environment changes to state s(cid:2). This is given by a tuple in",
            "the form of (s, a, r , s(cid:2)). State-action value which represents",
            "the expected total reward resulting from taking action a in",
            "state s is denoted by Q-value Q(s, a). The agent starts with",
            "random value and after each action they receive a tuple in the",
            "form of (s, a, r , s(cid:2)). For each tuple, the value of state-action",
            "could be calculated according to the following equation:"
        ],
        "paragraph_9": [
            "Q(s, a) = (1 − α)Q(s, a)",
            "+ α"
        ],
        "paragraph_10": [
            "(cid:2)",
            "r + γ max Q(s(cid:2), a(cid:2)) − Q(s, a)"
        ],
        "paragraph_11": [
            "(cid:3)"
        ],
        "paragraph_12": [
            "(1)"
        ],
        "paragraph_13": [
            "where α ∈ [0, 1] is the learning rate of agent. α = 1 means",
            "that merely new information is considered and zero means",
            "that the agent does not have any learning. γ ∈ [0, 1] is dis-",
            "count factor which determines future rewards. Zero value",
            "for this factor makes the agent opportunist which means that",
            "the agent only considers current reward. On the other hand,"
        ]
    },
    "page_3": {
        "paragraph_1": [
            "Arabian Journal for Science and Engineering"
        ],
        "paragraph_2": [
            "γ = 1 means that the agent will wait for a longer time to",
            "achieve a large reward. Q-learning will converge to optimum",
            "value Q∗(s, a) with probability of one if all state-action pairs",
            "are experienced repetitively and learning rate decrease during",
            "the time [22]. Generally, RL is useful for solving problems",
            "with small dimension discrete state and action space. When",
            "the dimension of state and action space becomes larger, the",
            "size of search table will be so large that it makes the algorithm",
            "very slow due to computational time. On the other hand, when",
            "the states or actions are stated continuously, using search",
            "table will not be possible. To tackle this problem, fuzzy the-",
            "ory is employed. If the intelligent agent has a proper fuzzy",
            "set as expert knowledge about the desired area, the ambigu-",
            "ity could be resolved. Thus, intelligent agent can understand",
            "vague objectives and unknown environment. In practice, the",
            "action in large spaces is facilitated by eliminating Q-values",
            "table. In this method everything is based on quality values",
            "and fuzzy inference. Fuzzy inference system (FIS) deals with",
            "input and Q-learning algorithm uses the follower section and",
            "its active rules as states. Reward signal of Q-algorithm is built",
            "in accordance with fuzzy logic, environment reward signal",
            "and performance estimation of current action. It is tried to",
            "select the action which maximizes the reward signal [9,14].",
            "Learning system is able to select one action among j actions",
            "for each rule. j-th possible action in i-th rule is denoted by",
            "a[i, j] and its value is shown by q[i, j] consider the follow-",
            "ing rules [9]:"
        ],
        "paragraph_3": [
            "If x is si"
        ],
        "paragraph_4": [
            "then a[i, 1] with q[i, 1]",
            "a[i, 2] with q[i, 2]",
            "or",
            "...",
            "or"
        ],
        "paragraph_5": [
            "a[i, j] with q[i, j]"
        ],
        "paragraph_6": [
            "Learning should ﬁnd the best result for each rule. If the agent",
            "selects an action which results in high value, it may learn",
            "optimum policy. Thus, fuzzy inference system may obtain",
            "necessary action for each rule [9]."
        ],
        "paragraph_7": [
            "3 Game Theory in ITS"
        ],
        "paragraph_8": [
            "Relation between agent-oriented environments and games",
            "theory originates from the fact that each state of agent-",
            "oriented environments can be resembled to a game environ-",
            "ment. Proﬁt function of players would be current state of the",
            "environment and goal of players is to move toward balanced",
            "or equilibrium point (reaching the best decision-making pol-",
            "icy). Some scholars have studied the application of Game",
            "theory to control of trafﬁc lights [15,16]. They integrate",
            "Game theory into the multi-agent interaction approach. Some",
            "of them suit the trafﬁc problem into a rigorous mathemati-",
            "cal game model [5,8,11], while others modify the learning"
        ],
        "paragraph_9": [
            "method of agents based on Game theory [33]. In [5], signal-",
            "ized intersections are modeled as ﬁnite controlled Markov",
            "chains and each intersection is seen as non-cooperative game",
            "where each player try to minimize its queue. The solutions",
            "are given as Nash equilibrium and Stackelberbg equilibrium",
            "and the simulation results indicate shorter queue length than",
            "adaptive control. In [8], a two-player non-cooperative game",
            "is articulated between user seeking a path to minimize the",
            "expected trip cost and choosing link performance scenarios",
            "to maximize the expected trip cost. It shows that the Nash",
            "equilibrium point measures network performance. Intelli-",
            "gent trafﬁc control is expressed as a Cournot game where the",
            "trafﬁc authority and the users choose their strategies simulta-",
            "neously and as a bi-level Stackelberg game where the trafﬁc",
            "authority is the leader which determines the signal settings in",
            "anticipation of the user reactions. In [33], Game theory is used",
            "to address coordination between agents based on trafﬁc signal",
            "control with Q-learning. It speciﬁes strategies (C(m) ={red",
            "light time plus 4 s, red light time plus 8 s, red light time minus",
            "4 s, red light time minus 8 s,unchangeably}) and actions",
            "(S(n) ={east west straight and right turn, south north straight",
            "and right turn, east west left turn, south north left turn}).",
            "Then, an interaction mathematical model via Game theory",
            "as a four parameter group G = {B, A, I , U } is presented.",
            "B is a group of decision-makers as players. A is a group of",
            "any possible strategies and actions, i.e. A = C(m) ∗ S(n).",
            "I represents the information which agents masters. U is the",
            "beneﬁt function which adopts Q-value. So, the Nash equi-",
            "librium is [33]:"
        ],
        "paragraph_10": [
            "Ui (a∗",
            "i"
        ],
        "paragraph_11": [
            ", a∗",
            "−i"
        ],
        "paragraph_12": [
            ") ≥ Ui (ai , a∗",
            "−i"
        ],
        "paragraph_13": [
            ")"
        ],
        "paragraph_14": [
            "(2)"
        ],
        "paragraph_15": [
            "(3)"
        ],
        "paragraph_16": [
            "where ai and a−i denote action of i-th agent and actions of",
            "other agents, respectively. a∗",
            "−i represent the actions",
            "at Nash equilibrium. The renewed Q-values in distributed",
            "reinforcement Q-learning are used to build the payoff values.",
            "Q-value function is updated as:"
        ],
        "paragraph_17": [
            "i and a∗"
        ],
        "paragraph_18": [
            "Qi (si , ai ) = (1 − αi )Qi (si , ai )"
        ],
        "paragraph_19": [
            "⎡"
        ],
        "paragraph_20": [
            "+ αi"
        ],
        "paragraph_21": [
            "⎣ri (si , ai ) +"
        ],
        "paragraph_22": [
            "n(cid:6)"
        ],
        "paragraph_23": [
            "j=1, j(cid:6)=i"
        ],
        "paragraph_24": [
            "f (i, j)r j (si , ai )"
        ],
        "paragraph_25": [
            "⎤"
        ],
        "paragraph_26": [
            "⎦"
        ],
        "paragraph_27": [
            "(4)"
        ],
        "paragraph_28": [
            "+ γ max(Qi (s(cid:2)",
            "i"
        ],
        "paragraph_29": [
            ", a(cid:2)",
            "i"
        ],
        "paragraph_30": [
            ") − Qi (si , ai ))"
        ],
        "paragraph_31": [
            "where α and γ are learning rate and discount factor, respec-",
            "tively. si and ai are current state of trafﬁc environment and",
            "current action, respectively. s(cid:2)",
            "i is its next state, n is the num-",
            "ber of trafﬁc signal control agents surrounding i-th agent,",
            "Qi (si , ai ) is the Q-value function for i-th agent when selects",
            "action ai in state si . ri (si , ai ) is reward function of i-th agent",
            "and r j (si , ai ) is reward function of j-th agent neighboring i-"
        ],
        "paragraph_32": [
            "123"
        ]
    },
    "page_4": {
        "paragraph_1": [
            "th agent. f (i, j) ∈ [0, 1] is a weighted function which shows",
            "the effect of r j (si , ai ) on i-th agent. Mathematical functions",
            "are suggested in [33] for r (s, a) and f (i, j). Assumption of",
            "discrete action-state space and determination of reward and",
            "weighting functions are drawbacks of that work."
        ],
        "paragraph_2": [
            "4 Problem Statements"
        ],
        "paragraph_3": [
            "Consider a trafﬁc network in which the lights of each inter-",
            "section is controlled by an autonomous agents without any",
            "centralized management. Some sensors which are installed",
            "below the surface of surrounding streets or trafﬁc cameras",
            "of each intersection provide information about trafﬁc situa-",
            "tion for the corresponding agent. An agent has to decide on",
            "duration of green light at north–south (NS) and west–east",
            "(WE) paths. Also, any agent interacts with neighbor agents.",
            "Anyway, the agent is expected to schedule trafﬁc lights opti-",
            "mally, in the sense of average delay, based on the received",
            "information from its sensors and received information from",
            "neighbor agents."
        ],
        "paragraph_4": [
            "The agents may have little knowledge about others’ deci-",
            "sion due to distribution of information. Even if an agent has",
            "previous known information about others’ decision, it is not",
            "valid as other agents are also learning. Thus, the environ-",
            "ment is dynamic and the behavior of other agents may change",
            "during time. Lack of prediction of other agents causes uncer-",
            "tainty in problem solving procedure. This paper looks for a",
            "decision-making algorithm for lights control agents which",
            "considers neighbor agents information in addition to its own",
            "information."
        ],
        "paragraph_5": [
            "5 Proposed Algorithm"
        ],
        "paragraph_6": [
            "We consider a constant duration T for green plus red phases.",
            "So, if the agent determines the green phase duration tg, then",
            "the red phase duration is tr = T − tg. Any typical agent i",
            "receives number of vehicles on the NS and WE streets from",
            "its own sensors and the green phase duration of neighbor",
            "agent j in order to schedule its own green phase duration.",
            "This paper proposes an autonomous agent with structure in",
            "Fig. 1 to control each intersection.",
            "The number of vehicles in WE and NS streets which are mea-",
            "sured by sensors are fuzziﬁed. Then, a fuzzy inference engine",
            "with rules as Eq. 2 are employed to ﬁre the corresponding",
            "output membership functions. Finally, defuzziﬁcation results",
            "to duration of green phase in NS path (t N S",
            "). Thus, the dura-",
            "= T −t N S",
            "tion of green phase in other path, WE, is t W E",
            ". We",
            "propose that, Q-value function which is updated by Eq. 4 be",
            "the value of each action in Eq. 2 which is denoted by q[i, j].",
            "This update equation takes the neighbor agents’ decision into",
            "account."
        ],
        "paragraph_7": [
            "g"
        ],
        "paragraph_8": [
            "g"
        ],
        "paragraph_9": [
            "g"
        ],
        "paragraph_10": [
            "123"
        ],
        "paragraph_11": [
            "Arabian Journal for Science and Engineering"
        ],
        "paragraph_12": [
            "Fig. 1 The proposed structure for a typical agent"
        ],
        "paragraph_13": [
            "The i-th agent takes decision of neighbor agent"
        ],
        "paragraph_14": [
            "j into",
            "account by reward r j (si , ai ) and a weighting function",
            "f (i, j). The reward is calculated based on average delay",
            "obtained from the decision made by the agent and current",
            "trafﬁc situation in a fuzzy manner. A fuzzy inference engine",
            "obtains these two inputs after fuzziﬁcation and gives the",
            "reward after defuzziﬁcation; see Fig. 1. weighting function",
            "f (i, j) ∈ [0, 1] shows the effect of r j (si , ai ) on the deci-",
            "sion of i-th agent. This weight is also calculated by a fuzzy",
            "inference engine. This engine takes its own tg, the neighbor",
            "agents’ tg, and number of waited vehicles and gives f (i, j).",
            "Suitable choice for reward and weighting function plays a",
            "signiﬁcant role in agent learning. The agent with structure in",
            "Fig. 1 runs the following algorithm:"
        ],
        "paragraph_15": [
            "1. Initial value of Qi -value for i-th trafﬁc signal control"
        ],
        "paragraph_16": [
            "agent is in the form of ∀(si , ai ) : Qi (si , ai ) = 0."
        ],
        "paragraph_17": [
            "2. Observing si by WE and NS sensors which is the current"
        ],
        "paragraph_18": [
            "state of i-th intersection."
        ],
        "paragraph_19": [
            "3. Selecting a proper estimation for desired state by fuzzy"
        ],
        "paragraph_20": [
            "inference system."
        ],
        "paragraph_21": [
            "4. Calculating the reward related to i-th and j-th trafﬁc",
            "signal control agent and the weighting function for neigh-",
            "boring agents separately.",
            "5. Observing new state s(cid:2)",
            "i .",
            "6. Updating Qi -value according to Eq. 4.",
            "7. Returning to step 2 till the variation of Q-value becomes"
        ],
        "paragraph_22": [
            "less than (cid:4)."
        ],
        "paragraph_23": [
            "6 Simulation Results"
        ],
        "paragraph_24": [
            "Consider a trafﬁc network with a center and four neighbor",
            "intersection. The delay in each intersection depends on phys-",
            "ical characteristics of the intersection, trafﬁc light scheduling",
            "and number of cars in input streets. We utilized trafﬁc model"
        ]
    },
    "page_5": {
        "paragraph_1": [
            "Arabian Journal for Science and Engineering"
        ],
        "paragraph_2": [
            "1"
        ],
        "paragraph_3": [
            "0.8"
        ],
        "paragraph_4": [
            "0.6"
        ],
        "paragraph_5": [
            "0.4"
        ],
        "paragraph_6": [
            "0.2"
        ],
        "paragraph_7": [
            "0"
        ],
        "paragraph_8": [
            "0"
        ],
        "paragraph_9": [
            "VL"
        ],
        "paragraph_10": [
            "L"
        ],
        "paragraph_11": [
            "M"
        ],
        "paragraph_12": [
            "H"
        ],
        "paragraph_13": [
            "VH"
        ],
        "paragraph_14": [
            "500"
        ],
        "paragraph_15": [
            "1000 1500 2000 2500 3000 3500"
        ],
        "paragraph_16": [
            "Number of vehivles"
        ],
        "paragraph_17": [
            "1"
        ],
        "paragraph_18": [
            "0.8"
        ],
        "paragraph_19": [
            "0.6"
        ],
        "paragraph_20": [
            "0.4"
        ],
        "paragraph_21": [
            "0.2"
        ],
        "paragraph_22": [
            "0"
        ],
        "paragraph_23": [
            "0"
        ],
        "paragraph_24": [
            "VS"
        ],
        "paragraph_25": [
            "S"
        ],
        "paragraph_26": [
            "M"
        ],
        "paragraph_27": [
            "H"
        ],
        "paragraph_28": [
            "VH"
        ],
        "paragraph_29": [
            "10"
        ],
        "paragraph_30": [
            "20"
        ],
        "paragraph_31": [
            "30",
            "Delay"
        ],
        "paragraph_32": [
            "40"
        ],
        "paragraph_33": [
            "50"
        ],
        "paragraph_34": [
            "60"
        ],
        "paragraph_35": [
            "Fig. 2 Membership function of number of vehicles enter the street for",
            "reward FIS"
        ],
        "paragraph_36": [
            "Fig. 3 Membership function of average delay for reward FIS"
        ],
        "paragraph_37": [
            "which is given by the American Highway Capacity Manual",
            "(HCM) [4, Eq.20]:"
        ],
        "paragraph_38": [
            "d = 0.38"
        ],
        "paragraph_39": [
            "C(1 − λ)2",
            "1 − x",
            "(cid:9)"
        ],
        "paragraph_40": [
            "+ 173x 2"
        ],
        "paragraph_41": [
            "(x − 1) +"
        ],
        "paragraph_42": [
            "(cid:11)"
        ],
        "paragraph_43": [
            "(cid:10)",
            "(x − 1)2 + 16x",
            "C"
        ],
        "paragraph_44": [
            "(5)"
        ],
        "paragraph_45": [
            "where d, C, λ, and x are average delay (s), cycle time (s),",
            "green ratio, and degree of saturation, respectively. λ = g",
            "c",
            "and x = v",
            "c , where c, g, and v are capacity (vehicle per hour),",
            "green time (sec), and input volume, respectively. We use this",
            "model to calculate average delay based on the green phase",
            "duration and number of vehicles. For more details of this",
            "equation, we refer to [4]."
        ],
        "paragraph_46": [
            "Assume that C = T = 100 s and c = 3500 veh/h. v is",
            "volume of vehicles entering each street which varies between",
            "0 to 3500 veh/h. g is duration of the green phase which each",
            "agent selects considering fuzzy Q-learning and interaction",
            "with adjacent agents. The trafﬁc network simulation algo-",
            "rithm is as follow:"
        ],
        "paragraph_47": [
            "1. The volume of vehicles entering each intersection (v) are",
            "randomly generated by a discrete uniform distribution on",
            "the interval [0, 3500]."
        ],
        "paragraph_48": [
            "2. Average delay is calculated by Eq. 5.",
            "3. Each agent decides on the time of green phase g.",
            "4. Go to step 1 until end of simulation time."
        ],
        "paragraph_49": [
            "Assume structure of the agents as in Fig. 1 with the Mam-",
            "dani FIS with input membership function as in Fig. 2 for",
            "number of input vehicles and Fig. 3 for average delay to",
            "calculate the reward functions r j (si , ai ). Centroid defuzzi-",
            "ﬁcation by the output membership function as in Fig. 4 is",
            "considered to estimate a reward value in interval [− 3, 3].",
            "The weighting function FIS has number of vehicles, its own",
            "green phase duration and the neighbor agents’ green phase",
            "duration as inputs. Figure 2 shows the membership function",
            "for number of vehicles, and Fig. 5 depicts the membership"
        ],
        "paragraph_50": [
            "VS"
        ],
        "paragraph_51": [
            "1"
        ],
        "paragraph_52": [
            "0.8"
        ],
        "paragraph_53": [
            "0.6"
        ],
        "paragraph_54": [
            "0.4"
        ],
        "paragraph_55": [
            "0.2"
        ],
        "paragraph_56": [
            "0"
        ],
        "paragraph_57": [
            "-3"
        ],
        "paragraph_58": [
            "S"
        ],
        "paragraph_59": [
            "M"
        ],
        "paragraph_60": [
            "H"
        ],
        "paragraph_61": [
            "VH"
        ],
        "paragraph_62": [
            "-2"
        ],
        "paragraph_63": [
            "-1"
        ],
        "paragraph_64": [
            "0",
            "Reward"
        ],
        "paragraph_65": [
            "1"
        ],
        "paragraph_66": [
            "2"
        ],
        "paragraph_67": [
            "3"
        ],
        "paragraph_68": [
            "Fig. 4 Membership function of output for reward FIS"
        ],
        "paragraph_69": [
            "1"
        ],
        "paragraph_70": [
            "0.8"
        ],
        "paragraph_71": [
            "0.6"
        ],
        "paragraph_72": [
            "0.4"
        ],
        "paragraph_73": [
            "0.2"
        ],
        "paragraph_74": [
            "0"
        ],
        "paragraph_75": [
            "0"
        ],
        "paragraph_76": [
            "L"
        ],
        "paragraph_77": [
            "M"
        ],
        "paragraph_78": [
            "H"
        ],
        "paragraph_79": [
            "20"
        ],
        "paragraph_80": [
            "40"
        ],
        "paragraph_81": [
            "60"
        ],
        "paragraph_82": [
            "80"
        ],
        "paragraph_83": [
            "100"
        ],
        "paragraph_84": [
            "Green phase duration"
        ],
        "paragraph_85": [
            "Fig. 5 Membership function of green phase duration for weighting",
            "function FIS"
        ],
        "paragraph_86": [
            "function for its own and neighbor green phase duration. Cen-",
            "troid defuzziﬁcation is applied to calculate weights on output",
            "membership function as in Fig. 6 which should be a value",
            "between 0 and 1.",
            "Finally, the agent uses fuzzy Q-learning (Eq. 2) with Q-value",
            "update rule (Eq. 4) where learning and discount factor are",
            "selected to be 0.5 and 0.7, respectively. The membership",
            "function for each measured number of vehicles is shown in",
            "Fig. 7. The output estimates green phase duration with mem-",
            "bership functions as in Fig. 8.",
            "The proposed method is compared with Fuzzy Q-learning",
            "(using Eq. 2 where q[i, j] is the Q-value which updates",
            "with Eq. 1), Q-learning (using Q-learning method with Q-",
            "value which updates with Eq. 1), fuzzy(using traditional",
            "fuzzy inference method) and ﬁxed time (tg = 60 s) in",
            "the sense of total average delay. Average delay in each"
        ],
        "paragraph_87": [
            "123"
        ]
    },
    "page_6": {
        "paragraph_1": [
            "VS"
        ],
        "paragraph_2": [
            "S"
        ],
        "paragraph_3": [
            "M"
        ],
        "paragraph_4": [
            "H"
        ],
        "paragraph_5": [
            "VH"
        ],
        "paragraph_6": [
            "Arabian Journal for Science and Engineering"
        ],
        "paragraph_7": [
            "0.2"
        ],
        "paragraph_8": [
            "0.4"
        ],
        "paragraph_9": [
            "0.6"
        ],
        "paragraph_10": [
            "0.8"
        ],
        "paragraph_11": [
            "1"
        ],
        "paragraph_12": [
            "Weight"
        ],
        "paragraph_13": [
            "1"
        ],
        "paragraph_14": [
            "0.8"
        ],
        "paragraph_15": [
            "0.6"
        ],
        "paragraph_16": [
            "0.4"
        ],
        "paragraph_17": [
            "0.2"
        ],
        "paragraph_18": [
            "0"
        ],
        "paragraph_19": [
            "0"
        ],
        "paragraph_20": [
            "Fig. 6 Membership function of output for weighting function FIS"
        ],
        "paragraph_21": [
            "1"
        ],
        "paragraph_22": [
            "0.8"
        ],
        "paragraph_23": [
            "0.6"
        ],
        "paragraph_24": [
            "0.4"
        ],
        "paragraph_25": [
            "0.2"
        ],
        "paragraph_26": [
            "0"
        ],
        "paragraph_27": [
            "0"
        ],
        "paragraph_28": [
            "VL"
        ],
        "paragraph_29": [
            "L"
        ],
        "paragraph_30": [
            "H"
        ],
        "paragraph_31": [
            "VH"
        ],
        "paragraph_32": [
            "500"
        ],
        "paragraph_33": [
            "1000 1500 2000 2500 3000 3500"
        ],
        "paragraph_34": [
            "Number of vehivles"
        ],
        "paragraph_35": [
            "Fig. 9 Delay of the proposed method, ﬁxed time, fuzzy Q-learning,",
            "Q-learning and fuzzy in each time step"
        ],
        "paragraph_36": [
            "Fig. 7 Membership function of number of vehicles for fuzzy Q-learning"
        ],
        "paragraph_37": [
            "1"
        ],
        "paragraph_38": [
            "0.8"
        ],
        "paragraph_39": [
            "0.6"
        ],
        "paragraph_40": [
            "0.4"
        ],
        "paragraph_41": [
            "0.2"
        ],
        "paragraph_42": [
            "0"
        ],
        "paragraph_43": [
            "0"
        ],
        "paragraph_44": [
            "S1"
        ],
        "paragraph_45": [
            "S2 S3 S4 S5 S6 S7 S8 S9"
        ],
        "paragraph_46": [
            "20"
        ],
        "paragraph_47": [
            "40"
        ],
        "paragraph_48": [
            "60"
        ],
        "paragraph_49": [
            "80"
        ],
        "paragraph_50": [
            "100"
        ],
        "paragraph_51": [
            "green phase duration"
        ],
        "paragraph_52": [
            "Fig. 10 Average of delay for the proposed method, ﬁxed time, fuzzy,",
            "Q-learning, fuzzy Q-learning"
        ],
        "paragraph_53": [
            "Fig. 8 Membership function of green phase duration for fuzzy Q-",
            "learning"
        ],
        "paragraph_54": [
            "time interval is depicted in Fig. 9, and the total average",
            "delay is illustrated in Fig. 10. The results illustrate that",
            "total average delay decrease from more than 50 s for ﬁxed",
            "time scheduling to approximately 15 s for the proposed",
            "method."
        ],
        "paragraph_55": [
            "The reward received from the neighbor and weighted func-",
            "tions of neighboring agents are factors learning algorithm.",
            "These parameters are fuzziﬁed through a FIS. Also, the",
            "number of vehicles in each street is measured and fuzzi-",
            "ﬁed to be used in decision-making process. The simulation",
            "results were compared with ﬁxed time method and other",
            "intelligent methods. The results revealed that our proposed",
            "method achieves considerable reduction of average delay in",
            "intersections."
        ],
        "paragraph_56": [
            "7 Conclusion"
        ],
        "paragraph_57": [
            "In this study, an intelligent control method of a controlling",
            "trafﬁc network was performed to decrease average delay",
            "time. Each trafﬁc light is considered as a learning agent.",
            "This paper proposed a structure for the agents. Each agent",
            "learn to decide on the duration of green phase through a",
            "fuzzy Q-learning algorithm which is modiﬁed by Game the-",
            "ory. Each agent receives a reward from neighbor agents."
        ],
        "paragraph_58": [
            "References"
        ],
        "paragraph_59": [
            "1. Abdoos, M.; Mozayani, N.; Bazzan, A.L.: Trafﬁc light control in",
            "non-stationary environments based on multi agent q-learning. In:",
            "14th International IEEE Conference on Intelligent Transportation",
            "Systems (ITSC), pp. 1580–1585. IEEE (2011)"
        ],
        "paragraph_60": [
            "2. Abdulhai, B.; Pringle, R.; Karakoulas, G.J.: Reinforcement learn-",
            "ing for true adaptive trafﬁc signal control. J. Transp. Eng. 129(3),",
            "278–285 (2003)"
        ],
        "paragraph_61": [
            "123"
        ]
    },
    "page_7": {
        "paragraph_1": [
            "Arabian Journal for Science and Engineering"
        ],
        "paragraph_2": [
            "3. Adler, J.L.; Satapathy, G.; Manikonda, V.; Bowles, B.; Blue, V.J.: A",
            "multi-agent approach to cooperative trafﬁc management and route",
            "guidance. Transp. Res. Part B Methodol. 39(4), 297–318 (2005)",
            "4. Akgungor, A.P.; Bullen, A.G.R.: Analytical delay models for sig-",
            "nalized intersections. In: 69th ITE Annual Meeting, Nevada, USA",
            "(1999)"
        ],
        "paragraph_3": [
            "5. Alvarez, I.; Poznyak, A.; Malo, A.: Urban trafﬁc control problem a",
            "game theory approach. In: 47th IEEE Conference on Decision and",
            "Control, pp. 2168–2172. IEEE (2008)"
        ],
        "paragraph_4": [
            "6. Balaji, P.; German, X.; Srinivasan, D.: Urban trafﬁc signal control",
            "using reinforcement learning agents. IET Intell. Transp. Syst. 4(3),",
            "177–188 (2010)"
        ],
        "paragraph_5": [
            "7. Bazzan, A.L.; Klgl, F.: A review on agent-based technology for traf-",
            "ﬁc and transportation. Knowl. Eng. Rev. 29(03), 375–403 (2014)",
            "8. Bell, M.G.: A game theory approach to measuring the performance",
            "reliability of transport networks. Transp. Res. Part B Methodol.",
            "34(6), 533–545 (2000)"
        ],
        "paragraph_6": [
            "9. Bonarini, A.; Lazaric, A.; Montrone, F.; Restelli, M.: Reinforce-",
            "ment distribution in fuzzy q-learning. Fuzzy Sets Syst. 160(10),",
            "1420–1443 (2009)"
        ],
        "paragraph_7": [
            "10. Bull, L.; ShaAban, J.; Tomlinson, A.; Addison, J.D.; Heydecker,",
            "B.G.: Towards distributed adaptive control for road trafﬁc junction",
            "signals using learning classiﬁer systems. In: Bull, L. (ed.) Applica-",
            "tions of Learning Classiﬁer Systems, pp. 276–299. Springer, Berlin",
            "(2004)"
        ],
        "paragraph_8": [
            "11. Chen, O.; Ben-Akiva, M.: Game-theoretic formulations of interac-",
            "tion between dynamic trafﬁc control and dynamic trafﬁc assign-",
            "ment. Transp. Res. Rec. J. Transp. Res. Board 1617, 179–188",
            "(1998)"
        ],
        "paragraph_9": [
            "12. Chin, Y.K.; Bolong, N.; Kiring, A.; Yang, S.S.; Teo, K.T.K.: Q-",
            "learning based trafﬁc optimization in management of signal timing",
            "plan. Int. J. Simul. Syst. Sci. Technol. 12(3), 29–35 (2011)"
        ],
        "paragraph_10": [
            "13. Da Silva, B.C.; Basso, E.W.; Perotto, F.S.; C Bazzan, A.L.; Engel,",
            "P.M.: Improving reinforcement learning with context detection.",
            "In: Proceedings of the Fifth International Joint Conference on",
            "Autonomous Agents and Multiagent Systems, pp. 810–812. ACM",
            "(2006)"
        ],
        "paragraph_11": [
            "14. Glowaty, G.: Enhancements of fuzzy q-learning algorithm. Com-"
        ],
        "paragraph_12": [
            "put. Sci. 7, 77–87 (2005)"
        ],
        "paragraph_13": [
            "15. Goyal, T.; Kaushal, S.: An intelligent scheduling scheme for real-",
            "time trafﬁc management using cooperative game theory and ahp-",
            "topsis methods for next generation telecommunication networks.",
            "Expert Syst. Appl. 86, 125–134 (2017)"
        ],
        "paragraph_14": [
            "16. Groot, N.; Zaccour, G.; De Schutter, B.: Hierarchical game the-",
            "ory for system-optimal control: applications of reverse stackelberg",
            "games in regulating marketing channels and trafﬁc routing. IEEE",
            "Control Syst. 37(2), 129–152 (2017)"
        ],
        "paragraph_15": [
            "17. Houli, D.; Zhiheng, L.; Yi, Z.: Multiobjective reinforcement learn-",
            "ing for trafﬁc signal control using vehicular ad hoc network.",
            "EURASIP J. Adv. Signal Process. 1, 724,035 (2010)"
        ],
        "paragraph_16": [
            "18. Iyer, V.; Jadhav, R.; Mavchi, U.; Abraham, J.: Intelligent trafﬁc",
            "signal synchronization using fuzzy logic and q-learning. In: Inter-",
            "national Conference on Computing, Analytics and Security Trends",
            "(CAST), pp. 156–161. IEEE (2016)"
        ],
        "paragraph_17": [
            "19. Kponyo, J.; Nwizege, K.; Opare, K.; Ahmed, A.; Hamdoun, H.;",
            "Akazua, L.; Alshehri, S.; Frank, H.: A distributed intelligent traf-",
            "ﬁc system using ant colony optimization: a netlogo modeling",
            "approach. In: International Conference on Systems Informatics,",
            "Modelling and Simulation (SIMS), pp. 11–17. IEEE (2016)",
            "20. Liu, Z.: A survey of intelligence methods in urban trafﬁc signal",
            "control. IJCSNS Int. J. Comput. Sci. Netw. Secur. 7(7), 105–112",
            "(2007)"
        ],
        "paragraph_18": [
            "21. Medina, J.C.; Hajbabaie, A.; Benekohal, R.F.: Arterial trafﬁc con-",
            "trol using reinforcement learning agents and information from",
            "adjacent intersections in the state and reward structure. In: 2010",
            "13th International IEEE Conference on Intelligent Transportation",
            "Systems (ITSC), pp. 525–530. IEEE (2010)"
        ],
        "paragraph_19": [
            "22. Pacheco, J.C.; Rossetti, R.J.: Agent-based trafﬁc control: a fuzzy",
            "q-learning approach. In: 13th International IEEE Conference on",
            "Intelligent Transportation Systems (ITSC), pp. 1172–1177. IEEE",
            "(2010)"
        ],
        "paragraph_20": [
            "23. Prashanth, L.; Bhatnagar, S.: Reinforcement learning with function",
            "approximation for trafﬁc signal control. IEEE Trans. Intell. Transp.",
            "Syst. 12(2), 412–421 (2011)"
        ],
        "paragraph_21": [
            "24. Rida, M.: Modeling and optimization of decision-making process",
            "during loading and unloading operations at container port. Arab. J.",
            "Sci. Eng. 39(11), 8395–8408 (2014)"
        ],
        "paragraph_22": [
            "25. Roess, R.P.; Prassas, E.S.; McShane, W.R.: Trafﬁc Engineering."
        ],
        "paragraph_23": [
            "Prentice Hall, Englewood Cliffs (2004)"
        ],
        "paragraph_24": [
            "26. Salkham, A.; Cunningham, R.; Garg, A.; Cahill, V.: A collaborative",
            "reinforcement learning approach to urban trafﬁc control optimiza-",
            "tion. In: Proceedings of IEEE/WIC/ACM International Conference",
            "on Web Intelligence and Intelligent Agent Technology, pp. 560–",
            "566. IEEE Computer Society (2008)"
        ],
        "paragraph_25": [
            "27. Schaefer, M.; Vokˇrínek, J.; Pinotti, D.; Tango, F.: Multi-agent trafﬁc",
            "simulation for development and validation of autonomic car-to-car",
            "systems. In: McCluskey, Th.L., Kotsialos, A., Müller, J.P., Klügl, F.,",
            "Rana, O., Schumann, R. (eds.) Autonomic Road Transport Support",
            "Systems, pp. 165–180. Springer, Berlin (2016)"
        ],
        "paragraph_26": [
            "28. Steingrover, M.; Schouten, R.; Peelen, S.; Nijhuis, E.; Bakker, B.:",
            "Reinforcement learning of trafﬁc light controllers adapting to trafﬁc",
            "congestion. In: BNAIC, pp. 216–223. Citeseer (2005)"
        ],
        "paragraph_27": [
            "29. Teknomo, K.: Application of microscopic pedestrian simulation",
            "model. Transp. Res. Part F Trafﬁc Psychol. Behav. 9(1), 15–27",
            "(2006)"
        ],
        "paragraph_28": [
            "30. Vilarinho, C.; Tavares, J.P.; Rossetti, R.J.: Intelligent trafﬁc lights:",
            "green time period negotiation. Transp. Res. Procedia 22, 325–334",
            "(2017)"
        ],
        "paragraph_29": [
            "31. Watkins, C.J.; Dayan, P.: Q-learning. Mach. Learn. 8(3–4), 279–"
        ],
        "paragraph_30": [
            "292 (1992)"
        ],
        "paragraph_31": [
            "32. Wiering, M.: Multi-agent reinforcement learning for trafﬁc light"
        ],
        "paragraph_32": [
            "control. In: ICML, pp. 1151–1158 (2000)"
        ],
        "paragraph_33": [
            "33. Xinhai, X.; Lunhui, X.: Trafﬁc signal control agent interaction",
            "model based on game theory and reinforcement learning. In:",
            "International Forum on Computer Science-Technology and Appli-",
            "cations, vol. 1, pp. 164–168. IEEE (2009)"
        ],
        "paragraph_34": [
            "123"
        ]
    }
}