Arabian Journal for Science and Engineering

γ = 1 means that the agent will wait for a longer time to
achieve a large reward. Q-learning will converge to optimum
value Q∗(s, a) with probability of one if all state-action pairs
are experienced repetitively and learning rate decrease during
the time [22]. Generally, RL is useful for solving problems
with small dimension discrete state and action space. When
the dimension of state and action space becomes larger, the
size of search table will be so large that it makes the algorithm
very slow due to computational time. On the other hand, when
the states or actions are stated continuously, using search
table will not be possible. To tackle this problem, fuzzy the-
ory is employed. If the intelligent agent has a proper fuzzy
set as expert knowledge about the desired area, the ambigu-
ity could be resolved. Thus, intelligent agent can understand
vague objectives and unknown environment. In practice, the
action in large spaces is facilitated by eliminating Q-values
table. In this method everything is based on quality values
and fuzzy inference. Fuzzy inference system (FIS) deals with
input and Q-learning algorithm uses the follower section and
its active rules as states. Reward signal of Q-algorithm is built
in accordance with fuzzy logic, environment reward signal
and performance estimation of current action. It is tried to
select the action which maximizes the reward signal [9,14].
Learning system is able to select one action among j actions
for each rule. j-th possible action in i-th rule is denoted by
a[i, j] and its value is shown by q[i, j] consider the follow-
ing rules [9]:

If x is si

then a[i, 1] with q[i, 1]
a[i, 2] with q[i, 2]
or
...
or

a[i, j] with q[i, j]

Learning should ﬁnd the best result for each rule. If the agent
selects an action which results in high value, it may learn
optimum policy. Thus, fuzzy inference system may obtain
necessary action for each rule [9].

3 Game Theory in ITS

Relation between agent-oriented environments and games
theory originates from the fact that each state of agent-
oriented environments can be resembled to a game environ-
ment. Proﬁt function of players would be current state of the
environment and goal of players is to move toward balanced
or equilibrium point (reaching the best decision-making pol-
icy). Some scholars have studied the application of Game
theory to control of trafﬁc lights [15,16]. They integrate
Game theory into the multi-agent interaction approach. Some
of them suit the trafﬁc problem into a rigorous mathemati-
cal game model [5,8,11], while others modify the learning

method of agents based on Game theory [33]. In [5], signal-
ized intersections are modeled as ﬁnite controlled Markov
chains and each intersection is seen as non-cooperative game
where each player try to minimize its queue. The solutions
are given as Nash equilibrium and Stackelberbg equilibrium
and the simulation results indicate shorter queue length than
adaptive control. In [8], a two-player non-cooperative game
is articulated between user seeking a path to minimize the
expected trip cost and choosing link performance scenarios
to maximize the expected trip cost. It shows that the Nash
equilibrium point measures network performance. Intelli-
gent trafﬁc control is expressed as a Cournot game where the
trafﬁc authority and the users choose their strategies simulta-
neously and as a bi-level Stackelberg game where the trafﬁc
authority is the leader which determines the signal settings in
anticipation of the user reactions. In [33], Game theory is used
to address coordination between agents based on trafﬁc signal
control with Q-learning. It speciﬁes strategies (C(m) ={red
light time plus 4 s, red light time plus 8 s, red light time minus
4 s, red light time minus 8 s,unchangeably}) and actions
(S(n) ={east west straight and right turn, south north straight
and right turn, east west left turn, south north left turn}).
Then, an interaction mathematical model via Game theory
as a four parameter group G = {B, A, I , U } is presented.
B is a group of decision-makers as players. A is a group of
any possible strategies and actions, i.e. A = C(m) ∗ S(n).
I represents the information which agents masters. U is the
beneﬁt function which adopts Q-value. So, the Nash equi-
librium is [33]:

Ui (a∗
i

, a∗
−i

) ≥ Ui (ai , a∗
−i

)

(2)

(3)

where ai and a−i denote action of i-th agent and actions of
other agents, respectively. a∗
−i represent the actions
at Nash equilibrium. The renewed Q-values in distributed
reinforcement Q-learning are used to build the payoff values.
Q-value function is updated as:

i and a∗

Qi (si , ai ) = (1 − αi )Qi (si , ai )

⎡

+ αi

⎣ri (si , ai ) +

n(cid:6)

j=1, j(cid:6)=i

f (i, j)r j (si , ai )

⎤

⎦

(4)

+ γ max(Qi (s(cid:2)
i

, a(cid:2)
i

) − Qi (si , ai ))

where α and γ are learning rate and discount factor, respec-
tively. si and ai are current state of trafﬁc environment and
current action, respectively. s(cid:2)
i is its next state, n is the num-
ber of trafﬁc signal control agents surrounding i-th agent,
Qi (si , ai ) is the Q-value function for i-th agent when selects
action ai in state si . ri (si , ai ) is reward function of i-th agent
and r j (si , ai ) is reward function of j-th agent neighboring i-

123

